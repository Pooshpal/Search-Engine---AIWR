{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>All imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import polars as pl\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Reading data from CSV into a pandas data frame</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv(\"Dataset/abcnews-date-text.csv\", encoding='utf-8')\n",
    "corpus.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pre processing of corpus</h2>\n",
    "<font color='grey'>Tokenizing, removing stop words, stemming or lemmatizing the words, and possibly other techniques like removing punctuation or normalizing capitalization.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all')\n",
    "\n",
    "# Tokenization\n",
    "corpus['headline_text'] = corpus['headline_text'].apply(lambda x: nltk.word_tokenize(x))   \n",
    "\n",
    "# Lowercasing\n",
    "corpus['headline_text'] = corpus['headline_text'].apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "# Remove punctuation\n",
    "corpus['headline_text'] = corpus['headline_text'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus['headline_text'] = corpus['headline_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus['headline_text'] = corpus['headline_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Join tokens back into sentences\n",
    "corpus['headline_text'] = corpus['headline_text'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Inverted Index</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10440/1964046889.py:11: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for idx, tokens in corpus['tokens'].iteritems():\n"
     ]
    }
   ],
   "source": [
    "corpus['tokens'] = corpus['headline_text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "corpus['tokens'] = corpus['tokens'].apply(lambda x: [word.lower() for word in x])\n",
    "corpus['tokens'] = corpus['tokens'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus['tokens'] = corpus['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus['tokens'] = corpus['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Build an inverted index\n",
    "inverted_index = {}\n",
    "for idx, tokens in corpus['tokens'].iteritems():\n",
    "    for token in tokens:\n",
    "        if token not in inverted_index:\n",
    "            inverted_index[token] = []\n",
    "        if idx not in inverted_index[token]:\n",
    "            inverted_index[token].append(idx)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Biword Index</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "biword_index = {}\n",
    "for index, tokens in corpus['tokens'].items():\n",
    "\n",
    "    # create pairs of consecutive words (biwords)and loop through them\n",
    "    biwords = [tokens[i] + ' ' + tokens[i+1] for i in range(len(tokens)-1)]\n",
    "    for i, biword in enumerate(biwords):\n",
    "        if biword in biword_index:\n",
    "            biword_index[biword].append((index, i))\n",
    "        else:\n",
    "            biword_index[biword] = [(index, i)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Query Pre Processing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(query):\n",
    "    # Tokenize query\n",
    "    query_terms = nltk.word_tokenize(query)\n",
    "\n",
    "    # Lowercasing\n",
    "    query_terms = list(word.lower() for word in query_terms)\n",
    "\n",
    "    # Remove punctuation\n",
    "    query_terms = list(word for word in query_terms if word not in string.punctuation)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    query_terms = list(word for word in query_terms if word not in stop_words)\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    query_terms = list(lemmatizer.lemmatize(word) for word in query_terms)\n",
    "\n",
    "    # Identify any multi-term phrases in the query\n",
    "    phrases = []\n",
    "    i = 0\n",
    "    while i < len(query_terms):\n",
    "        if i < len(query_terms) - 1 and query_terms[i+1] == ' '.join([query_terms[i], query_terms[i+1]]):\n",
    "            phrases.append(' '.join([query_terms[i], query_terms[i+1]]))\n",
    "            i += 2\n",
    "        else:\n",
    "            phrases.append(query_terms[i])\n",
    "            i += 1\n",
    "\n",
    "    return phrases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Boolean Retrieval</h2>\n",
    "<font color='grey'>Boolean retrieval model using the inverted index created above. It takes a user query string, tokenizes it, and uses the inverted index to find relevant documents that match the query terms using the boolean \"AND\" operator. It also supports multi-phrase queries by treating consecutive terms as a phrase and searching for that phrase in the corpus.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_search_inverted(query, inverted_index):\n",
    "    phrases = preprocess_query(query)\n",
    "\n",
    "    # Loop over query terms\n",
    "    for term in phrases:\n",
    "        if term not in inverted_index:\n",
    "            continue\n",
    "        \n",
    "        # Get set of documents that contain the term\n",
    "        docs_with_term = set(inverted_index[term])\n",
    "\n",
    "        # If first term in query, add all matching documents to set\n",
    "        if len(matching_docs) == 0:\n",
    "            matching_docs = docs_with_term\n",
    "        else:\n",
    "            # Intersection with previous matching documents\n",
    "            matching_docs = matching_docs.add(docs_with_term)\n",
    "            #matching_docs = matching_docs.intersection(docs_with_term)\n",
    "\n",
    "    # Return list of matching document ids\n",
    "    return list(matching_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def boolean_search(query, inverted_index,biword_index):   \n",
    "    query_terms = preprocess_query(query)\n",
    "    biword_query = []\n",
    "    for i in range(len(query_terms)-1):\n",
    "        biword_query.append(ps.stem(query_terms[i]) + ' ' + ps.stem(query_terms[i+1]))\n",
    "    relevant_docs = []\n",
    "    for biword in biword_query:\n",
    "        if biword in biword_index:\n",
    "            relevant_docs.extend(biword_index[biword])\n",
    "    relevant_docs = list(set(relevant_docs))\n",
    "    result_docs = defaultdict(int)\n",
    "    for doc_id in relevant_docs:\n",
    "        doc_dict = inverted_index[doc_id]\n",
    "        flag = True\n",
    "        for term in query_terms:\n",
    "            if term not in doc_dict:\n",
    "                flag = False\n",
    "                break\n",
    "        if flag:\n",
    "            result_docs[doc_id] = sum([doc_dict[term]['tf-idf'] for term in query_terms])\n",
    "    if len(result_docs) == 0:\n",
    "        return []\n",
    "    else:\n",
    "        return [k for k, v in sorted(result_docs.items(), key=lambda item: item[1], reverse=True)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Wildcard Queries</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wildcard_search(wildcard_query):\n",
    "    pattern = re.compile(wildcard_query.replace(\"*\", \".*\"))\n",
    "\n",
    "    matching_terms = [term for term in inverted_index.keys() if re.match(pattern, term)]\n",
    "\n",
    "    merged_posting_list = set()\n",
    "    for term in matching_terms:\n",
    "        posting_list = inverted_index[term]\n",
    "        merged_posting_list |= set(posting_list.keys())\n",
    "        return merged_posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wildcard_query = \"machine*\"\n",
    "res=wildcard_search(wildcard_query)\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Similarity Based Retreival</h2>\n",
    "<font color='grey'>Similarity based retrieval model that would need to calculate a similarity score between the user query and each document in the corpus. The documents could then be ranked based on their similarity scores, and the top results returned to the user. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query):\n",
    "    processed_query = ' '.join(preprocess_query(query))\n",
    "\n",
    "    # Initialize TF-IDF vectorizer & Create a list of all document texts\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    document_texts = corpus['headline_text'].tolist()\n",
    "\n",
    "    # Fit vectorizer on document texts & Transform query into a TF-IDF vector\n",
    "    vectorizer.fit(document_texts)\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "\n",
    "    # Transform all document texts into TF-IDF vectors & Calculate cosine similarity between query and documents\n",
    "    document_vectors = vectorizer.transform(document_texts)\n",
    "    similarity_scores = cosine_similarity(query_vector, document_vectors)\n",
    "    [similarity_score] = similarity_scores.tolist()\n",
    "    document_id = {i:similarity_score[i] for i in range(0,len(similarity_score))}\n",
    "    #print(document_id)\n",
    "    # Sort documents by similarity score in descending order\n",
    "    # Return sorted list of document IDs and corresponding similarity scores\n",
    "    sorted_dict = sorted(document_id.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_dict[:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Query for both models</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"climate changes\"\n",
    "results = boolean_search(query, inverted_index)\n",
    "#Top 10 reaches\n",
    "print(results[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"climate changes\"\n",
    "similar_documents = retrieve_documents(query)\n",
    "similar_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'climate change fight need political ardour'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.iloc[393216]['headline_text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Evaluation Of Metrics</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_engine(relevant_docs, retrieved_docs):\n",
    "\n",
    "    relevant_set = set(relevant_docs)\n",
    "    retrieved_set = set(retrieved_docs)\n",
    "    \n",
    "    #print('Relevant : ',sorted(relevant_set))\n",
    "    #print('Retrieved : ',sorted(retrieved_set))\n",
    "\n",
    "    true_positives = len(relevant_set.intersection(retrieved_set))\n",
    "    false_positives = len(retrieved_set - relevant_set)\n",
    "    false_negatives = len(relevant_set - retrieved_set)\n",
    "\n",
    "    print(\"TP: \",true_positives)\n",
    "    print(\"FP: \",false_positives)\n",
    "    print(\"FN: \",false_negatives)\n",
    "    \n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  246\n",
      "FP:  754\n",
      "FN:  754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.246, 0.246, 0.246)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_search_engine(list(similar_documents[i][0] for i in range(1000)),results[:1000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Advanced Search</h2>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Advanced Search Queries</h2>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Semantic Matching</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# define a function to calculate semantic similarity between two words using WordNet\n",
    "def calculate_similarity(word1, word2):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    if not synsets1 or not synsets2:\n",
    "        return 0.0\n",
    "    max_sim = -1\n",
    "    for synset1 in synsets1:\n",
    "        for synset2 in synsets2:\n",
    "            sim = wn.path_similarity(synset1, synset2)\n",
    "            if sim is not None and sim > max_sim:\n",
    "                max_sim = sim\n",
    "    return max_sim\n",
    "\n",
    "# define a function to perform semantic matching of a query against a document\n",
    "def semantic_matching(query):\n",
    "    matching_terms = []\n",
    "    for token in query.split():\n",
    "        if token in inverted_index:\n",
    "            matching_terms.append(token)\n",
    "\n",
    "    scores = []\n",
    "    for document in inverted_index:\n",
    "        doc_scores = []\n",
    "        for term in matching_terms:\n",
    "            if term in positional_index[document]:\n",
    "                doc_scores.append(max([calculate_similarity(term, doc_token) for doc_token in positional_index[document][term]]))\n",
    "            else:\n",
    "                doc_scores.append(0.0)\n",
    "        scores.append((document, sum(doc_scores)/len(matching_terms)))\n",
    "    return scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
